{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "https://github.com/bentrevett/pytorch-seq2seq/blob/main/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n"
   ],
   "id": "fb30eb2e86731cbf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:49:25.186145Z",
     "start_time": "2024-06-19T14:49:25.181407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import random"
   ],
   "id": "3ce9fe3e3e8391fa",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preparing data",
   "id": "dd7de0bb254d17cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:49:25.262167Z",
     "start_time": "2024-06-19T14:49:25.255156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {SOS_token: 'SOS', EOS_token: 'EOS'}\n",
    "        self.n_words = 2    # for SOS and EOS tokens\n",
    "        \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "            \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ],
   "id": "eb14a2e01b79c0f4",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:49:25.284252Z",
     "start_time": "2024-06-19T14:49:25.278180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ],
   "id": "d20b7825c4e55d9c",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:49:25.297360Z",
     "start_time": "2024-06-19T14:49:25.288265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_langs(lang1: str, lang2: str, reverse=False):\n",
    "    lines = open(\"data/%s-%s.txt\" % (lang1, lang2), encoding=\"utf-8\").readlines()\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        \n",
    "    return input_lang, output_lang, pairs"
   ],
   "id": "980b8a8a5f8d8b28",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:49:29.901142Z",
     "start_time": "2024-06-19T14:49:25.319368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_lang, output_lang, pairs = read_langs('eng', 'fra')\n",
    "for pair in pairs:\n",
    "    input_lang.add_sentence(pair[0])\n",
    "    output_lang.add_sentence(pair[1])"
   ],
   "id": "d57e9b41eac13710",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:49:29.907724Z",
     "start_time": "2024-06-19T14:49:29.903154Z"
    }
   },
   "cell_type": "code",
   "source": "MAX_LENGTH = 100",
   "id": "e63bc70e5261df5d",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Neural network module",
   "id": "258351ef669cdcf1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:49:29.915468Z",
     "start_time": "2024-06-19T14:49:29.908767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:49:29.926680Z",
     "start_time": "2024-06-19T14:49:29.917479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=encoder_outputs.device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        \n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            \n",
    "            if target_tensor is not None:\n",
    "                print(target_tensor[:, i].unsqueeze(1))\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "                \n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        \n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "    \n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output, hidden"
   ],
   "id": "37f2e1607632a667",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training and evaluation of this initial encoder-decoder",
   "id": "f00424fd9a568cc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:49:29.936326Z",
     "start_time": "2024-06-19T14:49:29.929698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensor_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "def tensors_from_pair(pair):\n",
    "    input_tensor = tensor_from_sentence(input_lang, pair[0])\n",
    "    target_tensor = tensor_from_sentence(output_lang, pair[1])\n",
    "    return input_tensor, target_tensor"
   ],
   "id": "a8e9589c659568eb",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:49:29.999685Z",
     "start_time": "2024-06-19T14:49:29.937337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hidden_size = 128\n",
    "encoder = Encoder(input_lang.n_words, hidden_size)\n",
    "decoder = Decoder(hidden_size, output_lang.n_words)"
   ],
   "id": "6c9a3da516d25230",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:49:30.226242Z",
     "start_time": "2024-06-19T14:49:30.001697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    training_examples = random.choices(pairs, k=5000)\n",
    "    \n",
    "    for p in training_examples:\n",
    "        i, t = tensors_from_pair(p)\n",
    "        \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "    \n",
    "        encoder_output, encoder_hidden = encoder(i)\n",
    "        print(t)\n",
    "        decoder_output, _, _ = decoder(encoder_output, encoder_hidden, t)\n",
    "        \n",
    "        loss = criterion(decoder_output, t)\n",
    "        loss.backward()\n",
    "        \n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "    "
   ],
   "id": "e92067d801d9fb0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 932],\n",
      "        [ 629],\n",
      "        [ 126],\n",
      "        [ 212],\n",
      "        [  62],\n",
      "        [1407],\n",
      "        [  37],\n",
      "        [   1]])\n",
      "tensor([[ 932],\n",
      "        [ 629],\n",
      "        [ 126],\n",
      "        [ 212],\n",
      "        [  62],\n",
      "        [1407],\n",
      "        [  37],\n",
      "        [   1]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden size (1, 8, 128), got [1, 7, 128]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[61], line 18\u001B[0m\n\u001B[0;32m     16\u001B[0m encoder_output, encoder_hidden \u001B[38;5;241m=\u001B[39m encoder(i)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(t)\n\u001B[1;32m---> 18\u001B[0m decoder_output, _, _ \u001B[38;5;241m=\u001B[39m decoder(encoder_output, encoder_hidden, t)\n\u001B[0;32m     20\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(decoder_output, t)\n\u001B[0;32m     21\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[58], line 15\u001B[0m, in \u001B[0;36mDecoder.forward\u001B[1;34m(self, encoder_outputs, encoder_hidden, target_tensor)\u001B[0m\n\u001B[0;32m     12\u001B[0m decoder_outputs \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(MAX_LENGTH):\n\u001B[1;32m---> 15\u001B[0m     decoder_output, decoder_hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward_step(decoder_input, decoder_hidden)\n\u001B[0;32m     16\u001B[0m     decoder_outputs\u001B[38;5;241m.\u001B[39mappend(decoder_output)\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m target_tensor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "Cell \u001B[1;32mIn[58], line 33\u001B[0m, in \u001B[0;36mDecoder.forward_step\u001B[1;34m(self, input, hidden)\u001B[0m\n\u001B[0;32m     31\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m     32\u001B[0m output \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(output)\n\u001B[1;32m---> 33\u001B[0m output, hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgru(output, hidden)\n\u001B[0;32m     34\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout(output)\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output, hidden\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1098\u001B[0m, in \u001B[0;36mGRU.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m   1093\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1094\u001B[0m         \u001B[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001B[39;00m\n\u001B[0;32m   1095\u001B[0m         \u001B[38;5;66;03m# the user believes he/she is passing in.\u001B[39;00m\n\u001B[0;32m   1096\u001B[0m         hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[1;32m-> 1098\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_forward_args(\u001B[38;5;28minput\u001B[39m, hx, batch_sizes)\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1100\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mgru(\u001B[38;5;28minput\u001B[39m, hx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers,\n\u001B[0;32m   1101\u001B[0m                      \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:276\u001B[0m, in \u001B[0;36mRNNBase.check_forward_args\u001B[1;34m(self, input, hidden, batch_sizes)\u001B[0m\n\u001B[0;32m    273\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_input(\u001B[38;5;28minput\u001B[39m, batch_sizes)\n\u001B[0;32m    274\u001B[0m expected_hidden_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_expected_hidden_size(\u001B[38;5;28minput\u001B[39m, batch_sizes)\n\u001B[1;32m--> 276\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_hidden_size(hidden, expected_hidden_size)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:259\u001B[0m, in \u001B[0;36mRNNBase.check_hidden_size\u001B[1;34m(self, hx, expected_hidden_size, msg)\u001B[0m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_hidden_size\u001B[39m(\u001B[38;5;28mself\u001B[39m, hx: Tensor, expected_hidden_size: Tuple[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mint\u001B[39m],\n\u001B[0;32m    257\u001B[0m                       msg: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected hidden size \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    258\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hx\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m!=\u001B[39m expected_hidden_size:\n\u001B[1;32m--> 259\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg\u001B[38;5;241m.\u001B[39mformat(expected_hidden_size, \u001B[38;5;28mlist\u001B[39m(hx\u001B[38;5;241m.\u001B[39msize())))\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected hidden size (1, 8, 128), got [1, 7, 128]"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:49:37.489012Z",
     "start_time": "2024-06-19T14:49:37.181562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input, target = tensors_from_pair(training_examples[0])\n",
    "output, hidden = encoder(input)\n",
    "output\n",
    "\n",
    "output.size(0)\n",
    "batch_size = output.size(0)\n",
    "decoder_input = torch.empty(batch_size, 1, dtype=torch.long).fill_(SOS_token)\n",
    "decoder_input\n",
    "\n",
    "decoder(output, hidden)"
   ],
   "id": "59fecdf101152761",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ -9.8524, -10.0098,  -9.6874,  ...,  -9.7908,  -9.8662, -10.2059],\n",
       "          [ -9.6220, -10.1778,  -9.6987,  ...,  -9.8464, -10.0043, -10.2546],\n",
       "          [ -9.8379, -10.1626,  -9.7662,  ...,  -9.8453, -10.0372, -10.2308],\n",
       "          ...,\n",
       "          [ -9.8341, -10.0373,  -9.8417,  ...,  -9.7817, -10.0032, -10.1979],\n",
       "          [ -9.8205,  -9.9820,  -9.8593,  ...,  -9.9877, -10.0537, -10.2796],\n",
       "          [ -9.8299,  -9.9611,  -9.8327,  ...,  -9.8110, -10.0717, -10.1962]],\n",
       " \n",
       "         [[ -9.8067,  -9.9969,  -9.8850,  ...,  -9.9026,  -9.8638, -10.2110],\n",
       "          [ -9.7569, -10.0952,  -9.8525,  ...,  -9.6568, -10.0844, -10.1839],\n",
       "          [ -9.7408, -10.1989,  -9.8038,  ...,  -9.7969,  -9.7845, -10.1629],\n",
       "          ...,\n",
       "          [-10.0139, -10.3328,  -9.7442,  ...,  -9.6685, -10.2844, -10.2150],\n",
       "          [-10.0139, -10.3328,  -9.7442,  ...,  -9.6685, -10.2844, -10.2150],\n",
       "          [-10.0139, -10.3328,  -9.7442,  ...,  -9.6685, -10.2844, -10.2150]],\n",
       " \n",
       "         [[ -9.8121, -10.1494,  -9.9172,  ...,  -9.9087,  -9.9108, -10.1984],\n",
       "          [ -9.9483,  -9.9998,  -9.8448,  ..., -10.0186,  -9.8360, -10.1483],\n",
       "          [ -9.7823, -10.1102,  -9.6432,  ...,  -9.9866,  -9.8272, -10.1452],\n",
       "          ...,\n",
       "          [-10.0139, -10.3328,  -9.7442,  ...,  -9.6685, -10.2844, -10.2150],\n",
       "          [-10.0139, -10.3328,  -9.7442,  ...,  -9.6685, -10.2844, -10.2150],\n",
       "          [-10.0139, -10.3328,  -9.7442,  ...,  -9.6685, -10.2844, -10.2150]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ -9.8143, -10.1535,  -9.7322,  ...,  -9.8669,  -9.8376, -10.1516],\n",
       "          [ -9.9306,  -9.8871,  -9.6006,  ...,  -9.8063,  -9.7603, -10.2026],\n",
       "          [ -9.8282, -10.0009,  -9.6391,  ...,  -9.8343,  -9.7930, -10.2202],\n",
       "          ...,\n",
       "          [ -9.7530,  -9.8394,  -9.7651,  ...,  -9.8570,  -9.9769, -10.1674],\n",
       "          [ -9.7763,  -9.8484,  -9.7545,  ...,  -9.9620,  -9.9353, -10.1008],\n",
       "          [ -9.8341, -10.0373,  -9.8417,  ...,  -9.7817, -10.0032, -10.1979]],\n",
       " \n",
       "         [[ -9.6662, -10.1118,  -9.7008,  ...,  -9.9019,  -9.9626, -10.3388],\n",
       "          [ -9.8761,  -9.8425,  -9.5823,  ...,  -9.8371,  -9.8620, -10.2737],\n",
       "          [ -9.9311, -10.0212,  -9.6090,  ...,  -9.7928,  -9.9956, -10.2628],\n",
       "          ...,\n",
       "          [-10.0139, -10.3328,  -9.7442,  ...,  -9.6685, -10.2844, -10.2150],\n",
       "          [-10.0139, -10.3328,  -9.7442,  ...,  -9.6685, -10.2844, -10.2150],\n",
       "          [-10.0139, -10.3328,  -9.7442,  ...,  -9.6685, -10.2844, -10.2150]],\n",
       " \n",
       "         [[ -9.7745, -10.1832,  -9.7847,  ...,  -9.9574,  -9.7768, -10.1878],\n",
       "          [ -9.9048,  -9.9077,  -9.6229,  ...,  -9.8625,  -9.7287, -10.2013],\n",
       "          [ -9.8140, -10.0010,  -9.6501,  ...,  -9.8660,  -9.7764, -10.2187],\n",
       "          ...,\n",
       "          [ -9.7763,  -9.8484,  -9.7545,  ...,  -9.9620,  -9.9353, -10.1008],\n",
       "          [ -9.8341, -10.0373,  -9.8417,  ...,  -9.7817, -10.0032, -10.1979],\n",
       "          [ -9.8205,  -9.9820,  -9.8593,  ...,  -9.9877, -10.0537, -10.2796]]],\n",
       "        grad_fn=<LogSoftmaxBackward0>),\n",
       " tensor([[[ 0.1011, -0.1423,  0.3406,  0.0553,  0.0890, -0.1845, -0.1455,\n",
       "            0.1498,  0.1344,  0.3291, -0.2274,  0.2724, -0.2148,  0.1207,\n",
       "           -0.0823, -0.2219, -0.0160,  0.1087,  0.1823,  0.2109, -0.1414,\n",
       "            0.2345,  0.2121, -0.1494, -0.0236, -0.1650,  0.2740,  0.1412,\n",
       "           -0.1188, -0.0817,  0.3574,  0.3095,  0.0849, -0.2376, -0.2862,\n",
       "           -0.0784,  0.6372,  0.1286,  0.3273, -0.0358, -0.0918, -0.2462,\n",
       "            0.1729, -0.0388,  0.1343, -0.3117, -0.3408, -0.0317,  0.0707,\n",
       "            0.0019, -0.5222,  0.4650, -0.1230,  0.1761, -0.1285,  0.2183,\n",
       "            0.3533, -0.2073,  0.0772,  0.1317, -0.2173,  0.2400,  0.2238,\n",
       "           -0.0101,  0.3926, -0.0935,  0.3637, -0.2149, -0.2085,  0.0363,\n",
       "           -0.3209, -0.2256, -0.1154,  0.6280, -0.2040,  0.0291,  0.2377,\n",
       "            0.2547, -0.3177, -0.2307, -0.2032,  0.0194,  0.1496, -0.2005,\n",
       "            0.0807,  0.0687,  0.3418,  0.1230, -0.2874, -0.4750,  0.3206,\n",
       "           -0.0545,  0.4351,  0.5012, -0.1561, -0.3638, -0.2383,  0.1700,\n",
       "           -0.1832, -0.0848,  0.1055,  0.1686, -0.0937,  0.0175, -0.2454,\n",
       "           -0.2299, -0.1309, -0.4289, -0.2810, -0.4828, -0.0690, -0.2319,\n",
       "            0.0048,  0.2322, -0.0944,  0.1910, -0.0589,  0.1217, -0.0943,\n",
       "            0.0570,  0.3951,  0.6606,  0.2565, -0.1154,  0.0659,  0.2087,\n",
       "           -0.3033, -0.2655],\n",
       "          [-0.2433, -0.7032, -0.0511, -0.3509,  0.3489,  0.0686, -0.1431,\n",
       "            0.2056, -0.5805,  0.3488,  0.4115,  0.5359,  0.6379,  0.0986,\n",
       "            0.1186, -0.3136,  0.2620,  0.4235,  0.3791,  0.0481, -0.0640,\n",
       "            0.1491,  0.1347,  0.0290,  0.5872, -0.5721,  0.2009, -0.1181,\n",
       "            0.6826, -0.7312,  0.1209, -0.4071, -0.3421,  0.1946,  0.6628,\n",
       "            0.4928,  0.4879,  0.2652, -0.6119, -0.0965, -0.0654,  0.2074,\n",
       "           -0.0927, -0.1986,  0.1916, -0.4544,  0.0236, -0.3688,  0.3830,\n",
       "           -0.0592,  0.1218,  0.5983,  0.6511,  0.1301, -0.3694,  0.1960,\n",
       "            0.3428,  0.0457, -0.6485, -0.1105,  0.2333,  0.1528,  0.6361,\n",
       "           -0.3938,  0.0256, -0.2194,  0.3583,  0.2457,  0.0520,  0.2433,\n",
       "           -0.0768, -0.1467, -0.5704,  0.6772, -0.5773, -0.2921,  0.0287,\n",
       "            0.3936,  0.5890, -0.2148, -0.6159,  0.0895,  0.1016,  0.1198,\n",
       "            0.0562,  0.2307,  0.4636,  0.5432,  0.4795, -0.0622,  0.5044,\n",
       "           -0.4256,  0.6261,  0.2424, -0.8110, -0.5032, -0.5136,  0.3445,\n",
       "            0.1297, -0.9074,  0.1789,  0.5443, -0.1123,  0.0363, -0.3020,\n",
       "           -0.2195,  0.3709, -0.5886,  0.1268, -0.1147,  0.1734,  0.0777,\n",
       "           -0.3561, -0.6372, -0.0765,  0.4260,  0.0019, -0.0633, -0.0768,\n",
       "           -0.2529,  0.8250,  0.6968,  0.2350,  0.1579,  0.7510,  0.6213,\n",
       "            0.5401, -0.3394],\n",
       "          [-0.2433, -0.7032, -0.0511, -0.3509,  0.3489,  0.0686, -0.1431,\n",
       "            0.2056, -0.5805,  0.3488,  0.4115,  0.5359,  0.6379,  0.0986,\n",
       "            0.1186, -0.3136,  0.2620,  0.4235,  0.3791,  0.0481, -0.0640,\n",
       "            0.1491,  0.1347,  0.0290,  0.5872, -0.5721,  0.2009, -0.1181,\n",
       "            0.6826, -0.7312,  0.1209, -0.4071, -0.3421,  0.1946,  0.6628,\n",
       "            0.4928,  0.4879,  0.2652, -0.6119, -0.0965, -0.0654,  0.2074,\n",
       "           -0.0927, -0.1986,  0.1916, -0.4544,  0.0236, -0.3688,  0.3830,\n",
       "           -0.0592,  0.1218,  0.5983,  0.6511,  0.1301, -0.3694,  0.1960,\n",
       "            0.3428,  0.0457, -0.6485, -0.1105,  0.2333,  0.1528,  0.6361,\n",
       "           -0.3938,  0.0256, -0.2194,  0.3583,  0.2457,  0.0520,  0.2433,\n",
       "           -0.0768, -0.1467, -0.5704,  0.6772, -0.5773, -0.2921,  0.0287,\n",
       "            0.3936,  0.5890, -0.2148, -0.6159,  0.0895,  0.1016,  0.1198,\n",
       "            0.0562,  0.2307,  0.4636,  0.5432,  0.4795, -0.0622,  0.5044,\n",
       "           -0.4256,  0.6261,  0.2424, -0.8110, -0.5032, -0.5136,  0.3445,\n",
       "            0.1297, -0.9074,  0.1789,  0.5443, -0.1123,  0.0363, -0.3020,\n",
       "           -0.2195,  0.3709, -0.5886,  0.1268, -0.1147,  0.1734,  0.0777,\n",
       "           -0.3561, -0.6372, -0.0765,  0.4260,  0.0019, -0.0633, -0.0768,\n",
       "           -0.2529,  0.8250,  0.6968,  0.2350,  0.1579,  0.7510,  0.6213,\n",
       "            0.5401, -0.3394],\n",
       "          [-0.2433, -0.7032, -0.0511, -0.3509,  0.3489,  0.0686, -0.1431,\n",
       "            0.2056, -0.5805,  0.3488,  0.4115,  0.5359,  0.6379,  0.0986,\n",
       "            0.1186, -0.3136,  0.2620,  0.4235,  0.3791,  0.0481, -0.0640,\n",
       "            0.1491,  0.1347,  0.0290,  0.5872, -0.5721,  0.2009, -0.1181,\n",
       "            0.6826, -0.7312,  0.1209, -0.4071, -0.3421,  0.1946,  0.6628,\n",
       "            0.4928,  0.4879,  0.2652, -0.6119, -0.0965, -0.0654,  0.2074,\n",
       "           -0.0927, -0.1986,  0.1916, -0.4544,  0.0236, -0.3688,  0.3830,\n",
       "           -0.0592,  0.1218,  0.5983,  0.6511,  0.1301, -0.3694,  0.1960,\n",
       "            0.3428,  0.0457, -0.6485, -0.1105,  0.2333,  0.1528,  0.6361,\n",
       "           -0.3938,  0.0256, -0.2194,  0.3583,  0.2457,  0.0520,  0.2433,\n",
       "           -0.0768, -0.1467, -0.5704,  0.6772, -0.5773, -0.2921,  0.0287,\n",
       "            0.3936,  0.5890, -0.2148, -0.6159,  0.0895,  0.1016,  0.1198,\n",
       "            0.0562,  0.2307,  0.4636,  0.5432,  0.4795, -0.0622,  0.5044,\n",
       "           -0.4256,  0.6261,  0.2424, -0.8110, -0.5032, -0.5136,  0.3445,\n",
       "            0.1297, -0.9074,  0.1789,  0.5443, -0.1123,  0.0363, -0.3020,\n",
       "           -0.2195,  0.3709, -0.5886,  0.1268, -0.1147,  0.1734,  0.0777,\n",
       "           -0.3561, -0.6372, -0.0765,  0.4260,  0.0019, -0.0633, -0.0768,\n",
       "           -0.2529,  0.8250,  0.6968,  0.2350,  0.1579,  0.7510,  0.6213,\n",
       "            0.5401, -0.3394],\n",
       "          [-0.2079, -0.0729,  0.0151,  0.0028, -0.0787, -0.3149, -0.0356,\n",
       "            0.5525,  0.1537,  0.4540, -0.1975,  0.1451, -0.2983, -0.0136,\n",
       "            0.1210,  0.1721,  0.0359, -0.2264, -0.3274,  0.1659, -0.1736,\n",
       "            0.3644, -0.2899, -0.0632,  0.1808, -0.0313,  0.3154,  0.2489,\n",
       "           -0.2528,  0.0804,  0.1597,  0.2320,  0.2861, -0.4813,  0.1481,\n",
       "           -0.1088,  0.4420, -0.1175, -0.0292, -0.1007,  0.4059, -0.1257,\n",
       "           -0.0381, -0.2940,  0.2681, -0.3098, -0.5826,  0.0862,  0.2411,\n",
       "            0.1129, -0.5031,  0.3618, -0.0806, -0.1982, -0.1349,  0.0484,\n",
       "            0.2648, -0.1132, -0.0524,  0.2323,  0.0477,  0.4329,  0.3257,\n",
       "            0.1790,  0.2394, -0.3074,  0.3771, -0.0420, -0.1253,  0.2565,\n",
       "           -0.2713, -0.0960,  0.2839,  0.4919, -0.2745,  0.0221, -0.1023,\n",
       "            0.1028, -0.1243, -0.3069,  0.0054, -0.5117,  0.3070, -0.1371,\n",
       "           -0.1362,  0.4310, -0.0087, -0.0552, -0.4243, -0.3438,  0.3288,\n",
       "           -0.2613,  0.3436,  0.4056, -0.0650, -0.3438,  0.0224,  0.1388,\n",
       "           -0.0574, -0.4680,  0.2692, -0.0604,  0.0563,  0.0276, -0.2902,\n",
       "            0.1171,  0.1337, -0.1038, -0.2430, -0.4252, -0.4563, -0.2875,\n",
       "           -0.1809,  0.2749, -0.1923,  0.3074, -0.2998,  0.3070, -0.0805,\n",
       "            0.4819,  0.0499,  0.6097,  0.4694, -0.2638,  0.0143,  0.0446,\n",
       "           -0.4503, -0.3562],\n",
       "          [-0.2433, -0.7032, -0.0511, -0.3509,  0.3489,  0.0686, -0.1431,\n",
       "            0.2056, -0.5805,  0.3488,  0.4115,  0.5359,  0.6379,  0.0986,\n",
       "            0.1186, -0.3136,  0.2620,  0.4235,  0.3791,  0.0481, -0.0640,\n",
       "            0.1491,  0.1347,  0.0290,  0.5872, -0.5721,  0.2009, -0.1181,\n",
       "            0.6826, -0.7312,  0.1209, -0.4071, -0.3421,  0.1946,  0.6628,\n",
       "            0.4928,  0.4879,  0.2652, -0.6119, -0.0965, -0.0654,  0.2074,\n",
       "           -0.0927, -0.1986,  0.1916, -0.4544,  0.0236, -0.3688,  0.3830,\n",
       "           -0.0592,  0.1218,  0.5983,  0.6511,  0.1301, -0.3694,  0.1960,\n",
       "            0.3428,  0.0457, -0.6485, -0.1105,  0.2333,  0.1528,  0.6361,\n",
       "           -0.3938,  0.0256, -0.2194,  0.3583,  0.2457,  0.0520,  0.2433,\n",
       "           -0.0768, -0.1467, -0.5704,  0.6772, -0.5773, -0.2921,  0.0287,\n",
       "            0.3936,  0.5890, -0.2148, -0.6159,  0.0895,  0.1016,  0.1198,\n",
       "            0.0562,  0.2307,  0.4636,  0.5432,  0.4795, -0.0622,  0.5044,\n",
       "           -0.4256,  0.6261,  0.2424, -0.8110, -0.5032, -0.5136,  0.3445,\n",
       "            0.1297, -0.9074,  0.1789,  0.5443, -0.1123,  0.0363, -0.3020,\n",
       "           -0.2195,  0.3709, -0.5886,  0.1268, -0.1147,  0.1734,  0.0777,\n",
       "           -0.3561, -0.6372, -0.0765,  0.4260,  0.0019, -0.0633, -0.0768,\n",
       "           -0.2529,  0.8250,  0.6968,  0.2350,  0.1579,  0.7510,  0.6213,\n",
       "            0.5401, -0.3394],\n",
       "          [ 0.1247, -0.0330,  0.2678,  0.0090,  0.1354, -0.1959,  0.0250,\n",
       "            0.5368,  0.3655,  0.1197, -0.3764,  0.1884, -0.2816,  0.0089,\n",
       "           -0.1834, -0.0489,  0.0027, -0.3217, -0.1946, -0.1806, -0.1213,\n",
       "            0.4262, -0.3160,  0.0296,  0.0983, -0.0245,  0.5575,  0.2806,\n",
       "           -0.3543, -0.1907,  0.1105,  0.2243, -0.0034,  0.1053, -0.2096,\n",
       "           -0.2566,  0.5208,  0.1579,  0.1912, -0.0033,  0.2081, -0.3047,\n",
       "           -0.2219, -0.0674,  0.0264, -0.4463, -0.5843,  0.0749,  0.2679,\n",
       "            0.1281, -0.6007,  0.3057,  0.1773, -0.0603,  0.0250,  0.2436,\n",
       "            0.2266, -0.3163,  0.1053,  0.1329, -0.1798,  0.4049,  0.3297,\n",
       "            0.0853,  0.3197, -0.0852,  0.3943, -0.2574, -0.2826,  0.0235,\n",
       "           -0.1798, -0.1755, -0.1387,  0.4825, -0.4717, -0.0589,  0.0918,\n",
       "            0.3228, -0.4121, -0.2989, -0.1451, -0.4140,  0.2604, -0.2236,\n",
       "           -0.0165,  0.3501,  0.1883, -0.0364, -0.2262, -0.6149, -0.2857,\n",
       "           -0.3496,  0.1372,  0.4387, -0.1081, -0.1464,  0.0418,  0.1952,\n",
       "           -0.3600, -0.1589,  0.4334, -0.1305,  0.1589,  0.2368, -0.3899,\n",
       "           -0.2421, -0.0661, -0.2616, -0.1623, -0.4475, -0.3172, -0.2145,\n",
       "            0.2299,  0.3934, -0.2621,  0.3859, -0.2404,  0.3840, -0.1071,\n",
       "            0.2683,  0.1917,  0.6422,  0.3759, -0.1875, -0.0531, -0.0527,\n",
       "           -0.3366, -0.4219]]], grad_fn=<StackBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "[tensors_from_pair(p)[0] for p in training_examples]",
   "id": "ce1e4090fc6b9352",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cccd530df1fe48f2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
