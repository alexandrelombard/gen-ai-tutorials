{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-11T12:54:02.811897Z",
     "start_time": "2024-06-11T12:53:59.647017Z"
    }
   },
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenization",
   "id": "5783cc0821339c27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:54:02.818969Z",
     "start_time": "2024-06-11T12:54:02.813896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = '''Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.'''\n",
    "\n",
    "def tokenize(text):\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "tokens = tokenize(text)"
   ],
   "id": "33aa66a51b89b76d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:54:02.836525Z",
     "start_time": "2024-06-11T12:54:02.820969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mapping(tokens):\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "    \n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "mapping(tokens)"
   ],
   "id": "314c4c17a090fc35",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'training': 0,\n",
       "  'are': 1,\n",
       "  'wide': 2,\n",
       "  'that': 3,\n",
       "  'perform': 4,\n",
       "  'on': 5,\n",
       "  'in': 6,\n",
       "  'a': 7,\n",
       "  'so': 8,\n",
       "  'without': 9,\n",
       "  'predictions': 10,\n",
       "  'sample': 11,\n",
       "  'intelligence': 12,\n",
       "  'to': 13,\n",
       "  'or': 14,\n",
       "  'explicitly': 15,\n",
       "  'tasks': 16,\n",
       "  'and': 17,\n",
       "  'of': 18,\n",
       "  'applications': 19,\n",
       "  'being': 20,\n",
       "  'mathematical': 21,\n",
       "  'data': 22,\n",
       "  'order': 23,\n",
       "  'improve': 24,\n",
       "  'subset': 25,\n",
       "  'such': 26,\n",
       "  'infeasible': 27,\n",
       "  'machine': 28,\n",
       "  'programmed': 29,\n",
       "  'filtering': 30,\n",
       "  'variety': 31,\n",
       "  'needed': 32,\n",
       "  'email': 33,\n",
       "  'conventional': 34,\n",
       "  'do': 35,\n",
       "  'decisions': 36,\n",
       "  'build': 37,\n",
       "  'seen': 38,\n",
       "  'model': 39,\n",
       "  'based': 40,\n",
       "  'is': 41,\n",
       "  'make': 42,\n",
       "  'automatically': 43,\n",
       "  'as': 44,\n",
       "  'learning': 45,\n",
       "  'study': 46,\n",
       "  'difficult': 47,\n",
       "  'vision': 48,\n",
       "  'used': 49,\n",
       "  'develop': 50,\n",
       "  'it': 51,\n",
       "  'where': 52,\n",
       "  'algorithms': 53,\n",
       "  'the': 54,\n",
       "  'artificial': 55,\n",
       "  'computer': 56,\n",
       "  'experience': 57,\n",
       "  'through': 58,\n",
       "  'known': 59},\n",
       " {0: 'training',\n",
       "  1: 'are',\n",
       "  2: 'wide',\n",
       "  3: 'that',\n",
       "  4: 'perform',\n",
       "  5: 'on',\n",
       "  6: 'in',\n",
       "  7: 'a',\n",
       "  8: 'so',\n",
       "  9: 'without',\n",
       "  10: 'predictions',\n",
       "  11: 'sample',\n",
       "  12: 'intelligence',\n",
       "  13: 'to',\n",
       "  14: 'or',\n",
       "  15: 'explicitly',\n",
       "  16: 'tasks',\n",
       "  17: 'and',\n",
       "  18: 'of',\n",
       "  19: 'applications',\n",
       "  20: 'being',\n",
       "  21: 'mathematical',\n",
       "  22: 'data',\n",
       "  23: 'order',\n",
       "  24: 'improve',\n",
       "  25: 'subset',\n",
       "  26: 'such',\n",
       "  27: 'infeasible',\n",
       "  28: 'machine',\n",
       "  29: 'programmed',\n",
       "  30: 'filtering',\n",
       "  31: 'variety',\n",
       "  32: 'needed',\n",
       "  33: 'email',\n",
       "  34: 'conventional',\n",
       "  35: 'do',\n",
       "  36: 'decisions',\n",
       "  37: 'build',\n",
       "  38: 'seen',\n",
       "  39: 'model',\n",
       "  40: 'based',\n",
       "  41: 'is',\n",
       "  42: 'make',\n",
       "  43: 'automatically',\n",
       "  44: 'as',\n",
       "  45: 'learning',\n",
       "  46: 'study',\n",
       "  47: 'difficult',\n",
       "  48: 'vision',\n",
       "  49: 'used',\n",
       "  50: 'develop',\n",
       "  51: 'it',\n",
       "  52: 'where',\n",
       "  53: 'algorithms',\n",
       "  54: 'the',\n",
       "  55: 'artificial',\n",
       "  56: 'computer',\n",
       "  57: 'experience',\n",
       "  58: 'through',\n",
       "  59: 'known'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Module",
   "id": "4516a3c9cfa603bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:54:02.844852Z",
     "start_time": "2024-06-11T12:54:02.838537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WordEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int):\n",
    "        super(WordEmbeddingModel, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(vocab_size, embedding_size)\n",
    "        self.linear2 = nn.Linear(embedding_size, vocab_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x) # The embedding is the output of linear1\n",
    "        x = self.linear2(x) \n",
    "        x = self.softmax(x)\n",
    "        return x"
   ],
   "id": "6ccee77d20efdb82",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "d8a4041142f2809a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:54:02.856275Z",
     "start_time": "2024-06-11T12:54:02.847863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_training_data(tokens, word_to_id, window):\n",
    "    X = []\n",
    "    y = []\n",
    "    n_tokens = len(tokens)\n",
    "    \n",
    "    for i in range(n_tokens):\n",
    "        idx = torch.concat((torch.Tensor(range(max(0, i - window), i)), torch.Tensor(range(i, min(n_tokens, i + window + 1)))))\n",
    "        for j in idx:\n",
    "            if i == j:\n",
    "                continue\n",
    "                \n",
    "            print(torch.Tensor(word_to_id[tokens[i]]))\n",
    "            X.append(F.one_hot(torch.Tensor([word_to_id[tokens[i]]]), len(word_to_id)))\n",
    "            y.append(F.one_hot(torch.Tensor([word_to_id[tokens[j]]]), len(word_to_id)))"
   ],
   "id": "c310c14f2d08bc0f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:54:02.877491Z",
     "start_time": "2024-06-11T12:54:02.858289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_to_id, id_to_word = mapping(tokens)\n",
    "m = WordEmbeddingModel(len(word_to_id), 10)"
   ],
   "id": "b559a5404f3f67a0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:54:05.022662Z",
     "start_time": "2024-06-11T12:54:02.878498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.05)"
   ],
   "id": "54c43797faf6ba3b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:54:06.537641Z",
     "start_time": "2024-06-11T12:54:05.024663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = generate_training_data(tokens, word_to_id, 2)\n",
    "\n",
    "for i in range(0, 5000):\n",
    "    pred_y = m(torch.tensor(X, dtype=torch.float))\n",
    "    loss = criterion(pred_y, torch.tensor(y, dtype=torch.float))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(m.parameters(), 5.0)\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ],
   "id": "7ee61c6951367413",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4678e-06, 1.4111e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one_hot is only applicable to index tensor.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m X, y \u001B[38;5;241m=\u001B[39m generate_training_data(tokens, word_to_id, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m5000\u001B[39m):\n\u001B[0;32m      4\u001B[0m     pred_y \u001B[38;5;241m=\u001B[39m m(torch\u001B[38;5;241m.\u001B[39mtensor(X, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat))\n",
      "Cell \u001B[1;32mIn[5], line 13\u001B[0m, in \u001B[0;36mgenerate_training_data\u001B[1;34m(tokens, word_to_id, window)\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(torch\u001B[38;5;241m.\u001B[39mTensor(word_to_id[tokens[i]]))\n\u001B[1;32m---> 13\u001B[0m X\u001B[38;5;241m.\u001B[39mappend(F\u001B[38;5;241m.\u001B[39mone_hot(torch\u001B[38;5;241m.\u001B[39mTensor([word_to_id[tokens[i]]]), \u001B[38;5;28mlen\u001B[39m(word_to_id)))\n\u001B[0;32m     14\u001B[0m y\u001B[38;5;241m.\u001B[39mappend(F\u001B[38;5;241m.\u001B[39mone_hot(torch\u001B[38;5;241m.\u001B[39mTensor([word_to_id[tokens[j]]]), \u001B[38;5;28mlen\u001B[39m(word_to_id)))\n",
      "\u001B[1;31mRuntimeError\u001B[0m: one_hot is only applicable to index tensor."
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "7d1c4718e2a3ef35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:54:35.104422Z",
     "start_time": "2024-06-11T12:54:35.099736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_embedding(model, word):\n",
    "    try:\n",
    "        idx = word_to_id[word]\n",
    "        one_hot = F.one_hot(torch.Tensor([idx]).to(torch.int64), len(word_to_id))\n",
    "        return model.linear1(one_hot.to(torch.float))\n",
    "    except KeyError:\n",
    "        print(\"`word` not in corpus\")"
   ],
   "id": "acd2aa1c55d17b1e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:54:37.286929Z",
     "start_time": "2024-06-11T12:54:37.254587Z"
    }
   },
   "cell_type": "code",
   "source": "machine_embedding = get_embedding(m, \"machine\")",
   "id": "4e493ad262ea064e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T14:28:29.012193Z",
     "start_time": "2024-06-12T14:28:28.993437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_distance = 10000\n",
    "closest_token = \"\"\n",
    "for t in word_to_id.keys():\n",
    "    with torch.no_grad():\n",
    "        embedding = get_embedding(m, t)\n",
    "        similarity = torch.dot(embedding.squeeze(), machine_embedding.squeeze()) / (torch.norm(machine_embedding, dim=1) * torch.norm(embedding, dim=1))\n",
    "        distance = torch.dist(embedding, machine_embedding).item()\n",
    "        print(t, \" - \", similarity.item())"
   ],
   "id": "9986e7ea16685fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training  -  -0.4389028251171112\n",
      "are  -  0.31396082043647766\n",
      "wide  -  0.37324926257133484\n",
      "that  -  0.35422223806381226\n",
      "perform  -  0.17575876414775848\n",
      "on  -  -0.08497027307748795\n",
      "in  -  -0.12576067447662354\n",
      "a  -  -0.07067960500717163\n",
      "so  -  0.1573219746351242\n",
      "without  -  0.11637531965970993\n",
      "predictions  -  0.04777004197239876\n",
      "sample  -  0.3522264063358307\n",
      "intelligence  -  0.2627631425857544\n",
      "to  -  -0.14298105239868164\n",
      "or  -  0.07358753681182861\n",
      "explicitly  -  -0.1310654878616333\n",
      "tasks  -  0.3535851538181305\n",
      "and  -  0.26100844144821167\n",
      "of  -  0.06291211396455765\n",
      "applications  -  0.1983295977115631\n",
      "being  -  0.10599274933338165\n",
      "mathematical  -  0.6067609786987305\n",
      "data  -  -0.14063821732997894\n",
      "order  -  0.27168992161750793\n",
      "improve  -  -0.25440409779548645\n",
      "subset  -  -0.20182788372039795\n",
      "such  -  -0.13383056223392487\n",
      "infeasible  -  -0.46990326046943665\n",
      "machine  -  1.0\n",
      "programmed  -  -0.1552230417728424\n",
      "filtering  -  0.058286793529987335\n",
      "variety  -  0.47354981303215027\n",
      "needed  -  0.2179267257452011\n",
      "email  -  -0.194364994764328\n",
      "conventional  -  0.21336333453655243\n",
      "do  -  0.7929871678352356\n",
      "decisions  -  0.5144434571266174\n",
      "build  -  0.05884142965078354\n",
      "seen  -  0.053757499903440475\n",
      "model  -  -0.13496695458889008\n",
      "based  -  -0.03835061565041542\n",
      "is  -  0.20651975274085999\n",
      "make  -  0.17463280260562897\n",
      "automatically  -  0.37785953283309937\n",
      "as  -  0.024014567956328392\n",
      "learning  -  0.15765608847141266\n",
      "study  -  0.22872044146060944\n",
      "difficult  -  -0.028032056987285614\n",
      "vision  -  -0.21087495982646942\n",
      "used  -  0.14714041352272034\n",
      "develop  -  -0.22344011068344116\n",
      "it  -  0.35605689883232117\n",
      "where  -  0.42793068289756775\n",
      "algorithms  -  0.10614552348852158\n",
      "the  -  0.09040644764900208\n",
      "artificial  -  0.3157235383987427\n",
      "computer  -  -0.2197030633687973\n",
      "experience  -  0.4179745018482208\n",
      "through  -  0.0907101035118103\n",
      "known  -  0.1447664052248001\n"
     ]
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
