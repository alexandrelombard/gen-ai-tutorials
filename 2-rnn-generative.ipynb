{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    "\n",
    "The goal is to generate names using the RNN architecture designed in the 1-RNN."
   ],
   "id": "d9967f3f4a8440e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "d3654bab604620c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:46:20.876051Z",
     "start_time": "2024-06-19T09:46:18.419588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "import typing\n",
    "import string\n",
    "import unicodedata"
   ],
   "id": "fd3005d709a2a2b4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading data",
   "id": "d9e671da25e83314"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:47:59.122287Z",
     "start_time": "2024-06-19T09:47:59.115249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters) + 1\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )"
   ],
   "id": "c574a49632cef830",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:48:01.350028Z",
     "start_time": "2024-06-19T09:48:01.299510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "categories = []\n",
    "category_lines = {}\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os.walk('data/names'):\n",
    "    for (i, filename) in enumerate([dirpath + os.sep + f for f in filenames]):\n",
    "        category = os.path.splitext(os.path.basename(filename))[0]\n",
    "        categories.append(category)\n",
    "        lines = open(filename, encoding='utf-8').read().lower().strip().split('\\n')\n",
    "        category_lines[category] = [unicode_to_ascii(line) for line in lines]\n",
    "        \n",
    "n_categories = len(category_lines)"
   ],
   "id": "263682020f101611",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:48:03.447078Z",
     "start_time": "2024-06-19T09:48:03.439990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def input_tensor(name: str) -> torch.Tensor:\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for (i, c) in enumerate(name):\n",
    "        tensor[i] = F.one_hot(torch.Tensor([all_letters.find(c)]).long(), num_classes=n_letters)\n",
    "        \n",
    "    return tensor"
   ],
   "id": "b280c6f3e4c5e193",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:49:01.210587Z",
     "start_time": "2024-06-19T09:49:01.205451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def category_to_tensor(category: str) -> torch.Tensor:\n",
    "    return F.one_hot(torch.Tensor([categories.index(category)]).long(), num_classes=n_categories)"
   ],
   "id": "6033ad998ed56c4a",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:48:06.829451Z",
     "start_time": "2024-06-19T09:48:06.824482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def target_tensor(name: str) -> torch.Tensor:\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for (i, c) in enumerate(name):\n",
    "        if i > 0:\n",
    "            tensor[i - 1] = F.one_hot(torch.Tensor([all_letters.find(c)]).long(), num_classes=n_letters)\n",
    "            \n",
    "    tensor[len(name) - 1] = F.one_hot(torch.Tensor([n_letters - 1]).long(), num_classes=n_letters)\n",
    "    \n",
    "    return tensor"
   ],
   "id": "b20856fcf3d23cd3",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Neural network module",
   "id": "fd733342a5874132"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:48:16.634349Z",
     "start_time": "2024-06-19T09:48:16.625609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_categories):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, category, input, hidden):\n",
    "        input_combined = torch.cat((category, input, hidden), dim=1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        output_combined = torch.cat((hidden, output), dim=1)\n",
    "        output = self.softmax(self.dropout(self.o2o(output_combined)))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ],
   "id": "75d64d8e5b694fa6",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:48:25.326926Z",
     "start_time": "2024-06-19T09:48:25.320479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_letters, len(categories))"
   ],
   "id": "ed21e59968315f3c",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "e8894f4fcffda363"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:48:28.830023Z",
     "start_time": "2024-06-19T09:48:28.824563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_training_example() -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    category = random.choice(categories)\n",
    "    line = random.choice(category_lines[category])\n",
    "    category_t = category_to_tensor(category)\n",
    "    line_t = input_tensor(line)\n",
    "    target_t = target_tensor(line)\n",
    "    return category_t, line_t, target_t"
   ],
   "id": "df001ebbbda0d41e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:48:30.879635Z",
     "start_time": "2024-06-19T09:48:30.871749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "n_iters = 100000\n",
    "learning_rate = 0.0005\n",
    "\n",
    "def train(category_tensor, input_tensor, target_tensor):\n",
    "    target_tensor.unsqueeze_(1)\n",
    "    hidden = rnn.init_hidden()\n",
    "    \n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    loss = torch.Tensor([0])\n",
    "    \n",
    "    for i in range(input_tensor.size(0)):\n",
    "        output, hidden = rnn(category_tensor, input_tensor[i], hidden)\n",
    "        l = criterion(output, torch.argmax(target_tensor[i].squeeze(0), dim=1))\n",
    "        loss += l\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "        \n",
    "    return output, loss.item() / input_tensor.size(0)"
   ],
   "id": "21274db236434b55",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T12:29:12.138064Z",
     "start_time": "2024-06-19T12:20:00.259945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "current_loss = 0\n",
    "all_losses = []\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category_tensor, line_tensor, target_t = get_training_example()\n",
    "    output, loss = train(category_tensor, line_tensor, target_t)\n",
    "    all_losses.append(loss)\n",
    "    if iter % 1000 == 0:\n",
    "        print(f'Iteration: {iter}, Average loss: {torch.Tensor(all_losses[-100:]).mean()}')\n",
    "    current_loss += loss"
   ],
   "id": "898351b4a028aa94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000, Average loss: 2.3202810287475586\n",
      "Iteration: 2000, Average loss: 2.334641933441162\n",
      "Iteration: 3000, Average loss: 2.217543125152588\n",
      "Iteration: 4000, Average loss: 2.333855152130127\n",
      "Iteration: 5000, Average loss: 2.3065688610076904\n",
      "Iteration: 6000, Average loss: 2.3336496353149414\n",
      "Iteration: 7000, Average loss: 2.3251264095306396\n",
      "Iteration: 8000, Average loss: 2.295895576477051\n",
      "Iteration: 9000, Average loss: 2.3418095111846924\n",
      "Iteration: 10000, Average loss: 2.306250810623169\n",
      "Iteration: 11000, Average loss: 2.3119490146636963\n",
      "Iteration: 12000, Average loss: 2.251181125640869\n",
      "Iteration: 13000, Average loss: 2.3523809909820557\n",
      "Iteration: 14000, Average loss: 2.303596019744873\n",
      "Iteration: 15000, Average loss: 2.27984881401062\n",
      "Iteration: 16000, Average loss: 2.385922431945801\n",
      "Iteration: 17000, Average loss: 2.279186248779297\n",
      "Iteration: 18000, Average loss: 2.2081010341644287\n",
      "Iteration: 19000, Average loss: 2.2171361446380615\n",
      "Iteration: 20000, Average loss: 2.158221483230591\n",
      "Iteration: 21000, Average loss: 2.2845871448516846\n",
      "Iteration: 22000, Average loss: 2.38179349899292\n",
      "Iteration: 23000, Average loss: 2.2413089275360107\n",
      "Iteration: 24000, Average loss: 2.315345048904419\n",
      "Iteration: 25000, Average loss: 2.214735507965088\n",
      "Iteration: 26000, Average loss: 2.229219675064087\n",
      "Iteration: 27000, Average loss: 2.2543342113494873\n",
      "Iteration: 28000, Average loss: 2.357494592666626\n",
      "Iteration: 29000, Average loss: 2.235156297683716\n",
      "Iteration: 30000, Average loss: 2.235988140106201\n",
      "Iteration: 31000, Average loss: 2.249676465988159\n",
      "Iteration: 32000, Average loss: 2.2118005752563477\n",
      "Iteration: 33000, Average loss: 2.2757513523101807\n",
      "Iteration: 34000, Average loss: 2.2803282737731934\n",
      "Iteration: 35000, Average loss: 2.2579033374786377\n",
      "Iteration: 36000, Average loss: 2.2659356594085693\n",
      "Iteration: 37000, Average loss: 2.2846264839172363\n",
      "Iteration: 38000, Average loss: 2.3675155639648438\n",
      "Iteration: 39000, Average loss: 2.3510031700134277\n",
      "Iteration: 40000, Average loss: 2.2765533924102783\n",
      "Iteration: 41000, Average loss: 2.305878162384033\n",
      "Iteration: 42000, Average loss: 2.261847972869873\n",
      "Iteration: 43000, Average loss: 2.2731575965881348\n",
      "Iteration: 44000, Average loss: 2.305389404296875\n",
      "Iteration: 45000, Average loss: 2.2713263034820557\n",
      "Iteration: 46000, Average loss: 2.188087224960327\n",
      "Iteration: 47000, Average loss: 2.343609094619751\n",
      "Iteration: 48000, Average loss: 2.2937352657318115\n",
      "Iteration: 49000, Average loss: 2.251577138900757\n",
      "Iteration: 50000, Average loss: 2.2777931690216064\n",
      "Iteration: 51000, Average loss: 2.3058505058288574\n",
      "Iteration: 52000, Average loss: 2.2471768856048584\n",
      "Iteration: 53000, Average loss: 2.2422165870666504\n",
      "Iteration: 54000, Average loss: 2.2198550701141357\n",
      "Iteration: 55000, Average loss: 2.3069417476654053\n",
      "Iteration: 56000, Average loss: 2.257068395614624\n",
      "Iteration: 57000, Average loss: 2.1556012630462646\n",
      "Iteration: 58000, Average loss: 2.3484151363372803\n",
      "Iteration: 59000, Average loss: 2.1798417568206787\n",
      "Iteration: 60000, Average loss: 2.313049554824829\n",
      "Iteration: 61000, Average loss: 2.2747514247894287\n",
      "Iteration: 62000, Average loss: 2.200423002243042\n",
      "Iteration: 63000, Average loss: 2.3162806034088135\n",
      "Iteration: 64000, Average loss: 2.2748987674713135\n",
      "Iteration: 65000, Average loss: 2.252807378768921\n",
      "Iteration: 66000, Average loss: 2.2685582637786865\n",
      "Iteration: 67000, Average loss: 2.221944808959961\n",
      "Iteration: 68000, Average loss: 2.2430546283721924\n",
      "Iteration: 69000, Average loss: 2.217151641845703\n",
      "Iteration: 70000, Average loss: 2.225255012512207\n",
      "Iteration: 71000, Average loss: 2.272588014602661\n",
      "Iteration: 72000, Average loss: 2.2738118171691895\n",
      "Iteration: 73000, Average loss: 2.2714080810546875\n",
      "Iteration: 74000, Average loss: 2.307692527770996\n",
      "Iteration: 75000, Average loss: 2.243241310119629\n",
      "Iteration: 76000, Average loss: 2.245759963989258\n",
      "Iteration: 77000, Average loss: 2.3441591262817383\n",
      "Iteration: 78000, Average loss: 2.209246873855591\n",
      "Iteration: 79000, Average loss: 2.0812790393829346\n",
      "Iteration: 80000, Average loss: 2.315669059753418\n",
      "Iteration: 81000, Average loss: 2.2392163276672363\n",
      "Iteration: 82000, Average loss: 2.1785831451416016\n",
      "Iteration: 83000, Average loss: 2.250291347503662\n",
      "Iteration: 84000, Average loss: 2.2622432708740234\n",
      "Iteration: 85000, Average loss: 2.2969565391540527\n",
      "Iteration: 86000, Average loss: 2.2261385917663574\n",
      "Iteration: 87000, Average loss: 2.26650071144104\n",
      "Iteration: 88000, Average loss: 2.31624698638916\n",
      "Iteration: 89000, Average loss: 2.2711331844329834\n",
      "Iteration: 90000, Average loss: 2.2436363697052\n",
      "Iteration: 91000, Average loss: 2.238013505935669\n",
      "Iteration: 92000, Average loss: 2.1471903324127197\n",
      "Iteration: 93000, Average loss: 2.2916297912597656\n",
      "Iteration: 94000, Average loss: 2.209468364715576\n",
      "Iteration: 95000, Average loss: 2.2244958877563477\n",
      "Iteration: 96000, Average loss: 2.2877068519592285\n",
      "Iteration: 97000, Average loss: 2.2657532691955566\n",
      "Iteration: 98000, Average loss: 2.2026941776275635\n",
      "Iteration: 99000, Average loss: 2.1973090171813965\n",
      "Iteration: 100000, Average loss: 2.2577247619628906\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fafef084e0892502"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a3db7f7a4c6ac8a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "b74ce2472f9bcefb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T12:32:44.752504Z",
     "start_time": "2024-06-19T12:32:44.708551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate(category, start_letter, max_length=20):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        category_tensor = category_to_tensor(category)\n",
    "        input = input_tensor(start_letter)\n",
    "        hidden = rnn.init_hidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = all_letters[topi.item()]\n",
    "                output_name += letter\n",
    "            input = input_tensor(letter)\n",
    "\n",
    "        return output_name\n",
    "\n",
    "# Now we can generate a name for a given category\n",
    "for start_letter in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'W', 'Z']:\n",
    "    print(generate('Arabic', start_letter))"
   ],
   "id": "200e26532c8f3768",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aasar\n",
      "Basar\n",
      "Casar\n",
      "Dosha\n",
      "Eosha\n",
      "Fasan\n",
      "Gasar\n",
      "Hasan\n",
      "Iasar\n",
      "Jasar\n",
      "Kasar\n",
      "Lasar\n",
      "Masar\n",
      "Nosha\n",
      "Oasar\n",
      "Pasar\n",
      "Rasar\n",
      "Sasan\n",
      "Tasar\n",
      "Uasan\n",
      "Wasar\n",
      "Zasar\n"
     ]
    }
   ],
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
