{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-10T12:53:59.799541Z",
     "start_time": "2024-06-10T12:53:57.245086Z"
    }
   },
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenization",
   "id": "5783cc0821339c27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T12:53:59.806968Z",
     "start_time": "2024-06-10T12:53:59.801550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = '''Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.'''\n",
    "\n",
    "def tokenize(text):\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "tokens = tokenize(text)"
   ],
   "id": "33aa66a51b89b76d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T12:54:40.569480Z",
     "start_time": "2024-06-10T12:54:40.560878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mapping(tokens):\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "    \n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "mapping(tokens)"
   ],
   "id": "314c4c17a090fc35",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'to': 0,\n",
       "  'algorithms': 1,\n",
       "  'artificial': 2,\n",
       "  'training': 3,\n",
       "  'vision': 4,\n",
       "  'it': 5,\n",
       "  'learning': 6,\n",
       "  'data': 7,\n",
       "  'difficult': 8,\n",
       "  'wide': 9,\n",
       "  'in': 10,\n",
       "  'predictions': 11,\n",
       "  'sample': 12,\n",
       "  'explicitly': 13,\n",
       "  'tasks': 14,\n",
       "  'the': 15,\n",
       "  'being': 16,\n",
       "  'make': 17,\n",
       "  'email': 18,\n",
       "  'where': 19,\n",
       "  'infeasible': 20,\n",
       "  'known': 21,\n",
       "  'without': 22,\n",
       "  'seen': 23,\n",
       "  'perform': 24,\n",
       "  'improve': 25,\n",
       "  'do': 26,\n",
       "  'a': 27,\n",
       "  'are': 28,\n",
       "  'through': 29,\n",
       "  'used': 30,\n",
       "  'needed': 31,\n",
       "  'such': 32,\n",
       "  'applications': 33,\n",
       "  'programmed': 34,\n",
       "  'of': 35,\n",
       "  'model': 36,\n",
       "  'filtering': 37,\n",
       "  'build': 38,\n",
       "  'computer': 39,\n",
       "  'is': 40,\n",
       "  'on': 41,\n",
       "  'variety': 42,\n",
       "  'study': 43,\n",
       "  'experience': 44,\n",
       "  'that': 45,\n",
       "  'order': 46,\n",
       "  'so': 47,\n",
       "  'and': 48,\n",
       "  'subset': 49,\n",
       "  'or': 50,\n",
       "  'automatically': 51,\n",
       "  'intelligence': 52,\n",
       "  'based': 53,\n",
       "  'decisions': 54,\n",
       "  'conventional': 55,\n",
       "  'mathematical': 56,\n",
       "  'machine': 57,\n",
       "  'develop': 58,\n",
       "  'as': 59},\n",
       " {0: 'to',\n",
       "  1: 'algorithms',\n",
       "  2: 'artificial',\n",
       "  3: 'training',\n",
       "  4: 'vision',\n",
       "  5: 'it',\n",
       "  6: 'learning',\n",
       "  7: 'data',\n",
       "  8: 'difficult',\n",
       "  9: 'wide',\n",
       "  10: 'in',\n",
       "  11: 'predictions',\n",
       "  12: 'sample',\n",
       "  13: 'explicitly',\n",
       "  14: 'tasks',\n",
       "  15: 'the',\n",
       "  16: 'being',\n",
       "  17: 'make',\n",
       "  18: 'email',\n",
       "  19: 'where',\n",
       "  20: 'infeasible',\n",
       "  21: 'known',\n",
       "  22: 'without',\n",
       "  23: 'seen',\n",
       "  24: 'perform',\n",
       "  25: 'improve',\n",
       "  26: 'do',\n",
       "  27: 'a',\n",
       "  28: 'are',\n",
       "  29: 'through',\n",
       "  30: 'used',\n",
       "  31: 'needed',\n",
       "  32: 'such',\n",
       "  33: 'applications',\n",
       "  34: 'programmed',\n",
       "  35: 'of',\n",
       "  36: 'model',\n",
       "  37: 'filtering',\n",
       "  38: 'build',\n",
       "  39: 'computer',\n",
       "  40: 'is',\n",
       "  41: 'on',\n",
       "  42: 'variety',\n",
       "  43: 'study',\n",
       "  44: 'experience',\n",
       "  45: 'that',\n",
       "  46: 'order',\n",
       "  47: 'so',\n",
       "  48: 'and',\n",
       "  49: 'subset',\n",
       "  50: 'or',\n",
       "  51: 'automatically',\n",
       "  52: 'intelligence',\n",
       "  53: 'based',\n",
       "  54: 'decisions',\n",
       "  55: 'conventional',\n",
       "  56: 'mathematical',\n",
       "  57: 'machine',\n",
       "  58: 'develop',\n",
       "  59: 'as'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Module",
   "id": "4516a3c9cfa603bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 39,
   "source": [
    "class WordEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int):\n",
    "        super(WordEmbeddingModel, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(vocab_size, embedding_size)\n",
    "        self.linear2 = nn.Linear(embedding_size, vocab_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x) # The embedding is the output of linear1\n",
    "        x = self.linear2(x) \n",
    "        x = self.softmax(x)\n",
    "        return x"
   ],
   "id": "6ccee77d20efdb82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "d8a4041142f2809a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 33,
   "source": [
    "def generate_training_data(tokens, word_to_id, window):\n",
    "    X = []\n",
    "    y = []\n",
    "    n_tokens = len(tokens)\n",
    "    \n",
    "    for i in range(n_tokens):\n",
    "        idx = torch.concat((torch.Tensor(range(max(0, i - window), i)), torch.Tensor(range(i, min(n_tokens, i + window + 1)))))\n",
    "        for j in idx:\n",
    "            if i == j:\n",
    "                continue\n",
    "                \n",
    "            print(torch.Tensor(word_to_id[tokens[i]]))\n",
    "            X.append(F.one_hot(torch.Tensor([word_to_id[tokens[i]]]), len(word_to_id)))\n",
    "            y.append(F.one_hot(torch.Tensor([word_to_id[tokens[j]]]), len(word_to_id)))"
   ],
   "id": "c310c14f2d08bc0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 44,
   "source": [
    "word_to_id, id_to_word = mapping(tokens)\n",
    "m = WordEmbeddingModel(len(word_to_id), 10)"
   ],
   "id": "b559a5404f3f67a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:18:52.732677Z",
     "start_time": "2024-06-10T13:18:49.649907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.05)"
   ],
   "id": "54c43797faf6ba3b",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:21:40.899061Z",
     "start_time": "2024-06-10T13:21:40.855303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = generate_training_data(tokens, word_to_id, 2)\n",
    "\n",
    "for i in range(0, 5000):\n",
    "    pred_y = m(torch.tensor(X, dtype=torch.float))\n",
    "    loss = criterion(pred_y, torch.tensor(y, dtype=torch.float))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(m.parameters(), 5.0)\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ],
   "id": "7ee61c6951367413",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2894e+38, 1.2219e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one_hot is only applicable to index tensor.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[45], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m X, y \u001B[38;5;241m=\u001B[39m generate_training_data(tokens, word_to_id, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m5000\u001B[39m):\n\u001B[0;32m      4\u001B[0m     pred_y \u001B[38;5;241m=\u001B[39m m(torch\u001B[38;5;241m.\u001B[39mtensor(X, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat))\n",
      "Cell \u001B[1;32mIn[33], line 13\u001B[0m, in \u001B[0;36mgenerate_training_data\u001B[1;34m(tokens, word_to_id, window)\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(torch\u001B[38;5;241m.\u001B[39mTensor(word_to_id[tokens[i]]))\n\u001B[1;32m---> 13\u001B[0m X\u001B[38;5;241m.\u001B[39mappend(F\u001B[38;5;241m.\u001B[39mone_hot(torch\u001B[38;5;241m.\u001B[39mTensor([word_to_id[tokens[i]]]), \u001B[38;5;28mlen\u001B[39m(word_to_id)))\n\u001B[0;32m     14\u001B[0m y\u001B[38;5;241m.\u001B[39mappend(F\u001B[38;5;241m.\u001B[39mone_hot(torch\u001B[38;5;241m.\u001B[39mTensor([word_to_id[tokens[j]]]), \u001B[38;5;28mlen\u001B[39m(word_to_id)))\n",
      "\u001B[1;31mRuntimeError\u001B[0m: one_hot is only applicable to index tensor."
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "7d1c4718e2a3ef35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T15:13:23.642827Z",
     "start_time": "2024-06-10T15:13:23.629992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_embedding(model, word):\n",
    "    try:\n",
    "        idx = word_to_id[word]\n",
    "        one_hot = F.one_hot(torch.Tensor([idx]).to(torch.int64), len(word_to_id))\n",
    "        return model.linear1(one_hot.to(torch.float))\n",
    "    except KeyError:\n",
    "        print(\"`word` not in corpus\")\n",
    "        \n",
    "get_embedding(m, \"machine\")"
   ],
   "id": "acd2aa1c55d17b1e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0521,  0.0279, -0.0518,  0.0839,  0.0105, -0.0985,  0.0078,  0.0740,\n",
       "          0.0148, -0.0250]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
